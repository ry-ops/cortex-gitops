apiVersion: v1
kind: ConfigMap
metadata:
  name: moe-router-app
  namespace: cortex-chat
data:
  app.py: |
    #!/usr/bin/env python3
    """
    MoE Router Service for Cortex Chat

    Routes chat requests to specialized experts based on intent classification.
    Integrates with Qdrant for vector storage and captures telemetry.
    """
    import os
    import json
    import yaml
    import logging
    import redis
    from datetime import datetime
    from flask import Flask, request, jsonify, Response
    from anthropic import Anthropic
    import requests
    from qdrant_client import QdrantClient
    from qdrant_client.models import Distance, VectorParams, PointStruct
    import hashlib

    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    logger = logging.getLogger(__name__)

    app = Flask(__name__)

    # Load configuration
    config_path = os.getenv('CONFIG_PATH', '/config/config.yaml')
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)

    # Initialize clients
    anthropic_client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))

    # Qdrant client
    qdrant_client = QdrantClient(
        host=config['qdrant']['host'],
        port=config['qdrant']['port']
    )

    # Redis client for telemetry
    if config['telemetry']['enabled']:
        redis_client = redis.Redis(
            host=config['telemetry']['redis_host'],
            port=config['telemetry']['redis_port'],
            decode_responses=True
        )
    else:
        redis_client = None

    # Initialize Qdrant collection
    COLLECTION_NAME = config['qdrant']['collection']

    def ensure_collection():
        """Ensure Qdrant collection exists"""
        try:
            collections = qdrant_client.get_collections().collections
            if not any(c.name == COLLECTION_NAME for c in collections):
                qdrant_client.create_collection(
                    collection_name=COLLECTION_NAME,
                    vectors_config=VectorParams(size=1536, distance=Distance.COSINE)
                )
                logger.info(f"Created Qdrant collection: {COLLECTION_NAME}")
        except Exception as e:
            logger.error(f"Error ensuring collection: {e}")

    ensure_collection()

    def classify_intent(message: str) -> dict:
        """
        Classify user intent using Claude
        Returns dict with expert, confidence, and reasoning
        """
        experts_desc = "\n".join([
            f"- {e['name']}: {e['specialization']}"
            for e in config['router']['experts']
        ])

        prompt = f"""Classify the user's intent and select the best expert to handle their request.

Available experts:
{experts_desc}

User message: {message}

Respond with JSON only:
{{
  "expert": "expert_name",
  "confidence": 0.0-1.0,
  "reasoning": "brief explanation"
}}"""

        try:
            response = anthropic_client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=200,
                messages=[{"role": "user", "content": prompt}]
            )

            result = json.loads(response.content[0].text)
            return result
        except Exception as e:
            logger.error(f"Intent classification error: {e}")
            return {"expert": "general", "confidence": 0.5, "reasoning": "fallback"}

    def get_context_from_qdrant(message: str, conversation_id: str, limit: int = 5) -> list:
        """Retrieve relevant context from Qdrant"""
        try:
            # Generate embedding using Claude (simplified - would use proper embedding model)
            # For now, just search by conversation_id metadata
            results = qdrant_client.scroll(
                collection_name=COLLECTION_NAME,
                scroll_filter={
                    "must": [
                        {"key": "conversation_id", "match": {"value": conversation_id}}
                    ]
                },
                limit=limit
            )

            return [point.payload for point, _ in results[0]] if results else []
        except Exception as e:
            logger.error(f"Qdrant context retrieval error: {e}")
            return []

    def store_in_qdrant(message: str, response: str, conversation_id: str, metadata: dict):
        """Store conversation in Qdrant"""
        try:
            # Generate simple hash-based vector (would use proper embeddings in production)
            vector = [float(ord(c) % 256) / 256.0 for c in hashlib.sha512(message.encode()).hexdigest()[:1536]]

            point = PointStruct(
                id=hashlib.md5(f"{conversation_id}-{datetime.utcnow().isoformat()}".encode()).hexdigest(),
                vector=vector,
                payload={
                    "conversation_id": conversation_id,
                    "message": message,
                    "response": response,
                    "timestamp": datetime.utcnow().isoformat(),
                    **metadata
                }
            )

            qdrant_client.upsert(
                collection_name=COLLECTION_NAME,
                points=[point]
            )
            logger.info(f"Stored conversation in Qdrant: {conversation_id}")
        except Exception as e:
            logger.error(f"Qdrant storage error: {e}")

    def capture_telemetry(intent: dict, expert: str, latency: float, success: bool):
        """Capture routing telemetry to Redis"""
        if not redis_client:
            return

        try:
            key_prefix = config['telemetry']['redis_key_prefix']
            timestamp = datetime.utcnow().isoformat()

            # Store routing decision
            telemetry = {
                "timestamp": timestamp,
                "intent": intent,
                "expert": expert,
                "latency_ms": latency,
                "success": success
            }

            # Store as sorted set for time-series analysis
            redis_client.zadd(
                f"{key_prefix}routing_decisions",
                {json.dumps(telemetry): datetime.utcnow().timestamp()}
            )

            # Increment expert usage counter
            redis_client.hincrby(f"{key_prefix}expert_usage", expert, 1)

            logger.info(f"Captured telemetry for expert: {expert}")
        except Exception as e:
            logger.error(f"Telemetry capture error: {e}")

    @app.route('/health', methods=['GET'])
    def health():
        """Health check endpoint"""
        return jsonify({"status": "healthy", "service": "moe-router"}), 200

    @app.route('/chat', methods=['POST'])
    def chat():
        """
        Main chat endpoint
        Request: {message: str, conversation_id: str}
        """
        try:
            data = request.json
            message = data.get('message', '')
            conversation_id = data.get('conversation_id', 'default')

            start_time = datetime.utcnow()

            # Step 1: Classify intent
            intent = classify_intent(message)
            expert_name = intent['expert']

            # Step 2: Get relevant context from Qdrant
            context = get_context_from_qdrant(message, conversation_id)

            # Step 3: Find expert endpoint
            expert = next((e for e in config['router']['experts'] if e['name'] == expert_name), None)
            if not expert:
                expert = config['router']['experts'][0]  # Fallback to first expert

            # Step 4: Route to expert
            expert_request = {
                "message": message,
                "conversation_id": conversation_id,
                "context": context,
                "intent": intent
            }

            try:
                response = requests.post(
                    f"{expert['endpoint']}/chat",
                    json=expert_request,
                    timeout=30
                )

                if response.status_code == 200:
                    expert_response = response.json()

                    # Step 5: Store in Qdrant
                    store_in_qdrant(
                        message,
                        expert_response.get('response', ''),
                        conversation_id,
                        {"expert": expert_name, "intent": intent}
                    )

                    # Step 6: Capture telemetry
                    latency = (datetime.utcnow() - start_time).total_seconds() * 1000
                    capture_telemetry(intent, expert_name, latency, True)

                    return jsonify({
                        "response": expert_response.get('response', ''),
                        "expert": expert_name,
                        "confidence": intent['confidence'],
                        "latency_ms": latency
                    }), 200
                else:
                    raise Exception(f"Expert returned {response.status_code}")

            except Exception as e:
                logger.error(f"Expert routing error: {e}")
                latency = (datetime.utcnow() - start_time).total_seconds() * 1000
                capture_telemetry(intent, expert_name, latency, False)

                return jsonify({
                    "error": "Expert routing failed",
                    "message": str(e)
                }), 500

        except Exception as e:
            logger.error(f"Chat error: {e}")
            return jsonify({"error": str(e)}), 500

    @app.route('/telemetry', methods=['GET'])
    def get_telemetry():
        """Get routing telemetry stats"""
        if not redis_client:
            return jsonify({"error": "Telemetry disabled"}), 404

        try:
            key_prefix = config['telemetry']['redis_key_prefix']

            # Get expert usage stats
            expert_usage = redis_client.hgetall(f"{key_prefix}expert_usage")

            # Get recent routing decisions (last 100)
            recent_decisions = redis_client.zrange(
                f"{key_prefix}routing_decisions",
                -100, -1
            )

            return jsonify({
                "expert_usage": expert_usage,
                "recent_decisions_count": len(recent_decisions)
            }), 200
        except Exception as e:
            logger.error(f"Telemetry retrieval error: {e}")
            return jsonify({"error": str(e)}), 500

    if __name__ == '__main__':
        app.run(host='0.0.0.0', port=8080)

  requirements.txt: |
    flask==3.0.0
    anthropic==0.39.0
    requests==2.31.0
    redis==5.0.1
    qdrant-client==1.7.0
    pyyaml==6.0.1

  Dockerfile: |
    FROM python:3.11-slim

    WORKDIR /app

    COPY requirements.txt .
    RUN pip install --no-cache-dir -r requirements.txt

    COPY app.py .

    USER 1000

    CMD ["python", "app.py"]
---
apiVersion: batch/v1
kind: Job
metadata:
  name: kaniko-moe-router
  namespace: cortex-chat
spec:
  ttlSecondsAfterFinished: 600
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: kaniko
        image: gcr.io/kaniko-project/executor:latest
        args:
        - "--dockerfile=/workspace/Dockerfile"
        - "--context=dir:///workspace"
        - "--destination=10.43.170.72:5000/moe-router:latest"
        - "--insecure"
        - "--skip-tls-verify"
        volumeMounts:
        - name: app-code
          mountPath: /workspace
      volumes:
      - name: app-code
        configMap:
          name: moe-router-app
